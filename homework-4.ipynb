{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### \ud83c\udf93 **Professor**: Apostolos Filippas\n",
    "\n",
    "### \ud83d\udcd8 **Class**: AI Engineering\n",
    "\n",
    "### \ud83d\udccb **Homework 4**: Embeddings & Semantic Search\n",
    "\n",
    "### \ud83d\udcc5 **Due Date**: Day of Lecture 5, 11:59 PM\n",
    "\n",
    "\n",
    "**Note**: You are not allowed to share the contents of this notebook with anyone outside this class without written permission by the professor.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this homework, you'll build on Homework 3 (BM25 search) by adding **embedding-based semantic search**.\n",
    "\n",
    "You will:\n",
    "1. **Generate embeddings** using both local (Hugging Face) and API (OpenAI) models\n",
    "2. **Implement cosine similarity** from scratch\n",
    "3. **Implement semantic search** from scratch\n",
    "4. **Compare BM25 vs semantic search** using Recall\n",
    "5. **Compare different embedding models** and analyze their differences\n",
    "\n",
    "**Total Points: 95**\n",
    "\n",
    "---\n",
    "\n",
    "## Instructions\n",
    "\n",
    "- Complete all tasks by filling in code where you see `# YOUR CODE HERE`\n",
    "- You may use ChatGPT, Claude, documentation, Stack Overflow, etc.\n",
    "- When using external resources, briefly cite them in a comment\n",
    "- Run all cells before submitting to ensure they work\n",
    "\n",
    "**Submission:**\n",
    "1. Create a branch called `homework-4`\n",
    "2. Commit and push your work\n",
    "3. Create a PR and merge to main\n",
    "4. Submit the `.ipynb` file on Blackboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 1: Environment Setup (10 points)\n",
    "\n",
    "### 1a. Imports (5 pts)\n",
    "\n",
    "Import the required libraries and load the WANDS data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ruff: noqa: E402\n",
    "\n",
    "# Standard imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Import ONLY data loading from helpers\n",
    "import sys\n",
    "sys.path.append('../scripts')\n",
    "from helpers import load_wands_products, load_wands_queries, load_wands_labels\n",
    "\n",
    "# Embedding libraries - we use these directly\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import litellm\n",
    "\n",
    "# Load environment variables for API keys\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "pd.set_option('display.max_colwidth', 80)\n",
    "print(\"All imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the WANDS dataset\n",
    "products = load_wands_products()\n",
    "queries = load_wands_queries()\n",
    "labels = load_wands_labels()\n",
    "\n",
    "print(f\"Products: {len(products):,}\")\n",
    "print(f\"Queries: {len(queries):,}\")\n",
    "print(f\"Labels: {len(labels):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1b. Copy BM25 functions from HW3 (5 pts)\n",
    "\n",
    "Copy your BM25 implementation from Homework 3. We'll use it to compare against semantic search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy your BM25 functions from Homework 3\n",
    "import Stemmer\n",
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "stemmer = Stemmer.Stemmer('english')\n",
    "punct_trans = str.maketrans({key: ' ' for key in string.punctuation})\n",
    "\n",
    "def snowball_tokenize(text: str) -> list[str]:\n",
    "    \"\"\"Tokenize text with Snowball stemming.\"\"\"\n",
    "    if pd.isna(text) or text is None:\n",
    "        return []\n",
    "    text = str(text).translate(punct_trans)\n",
    "    tokens = text.lower().split()\n",
    "    return [stemmer.stemWord(token) for token in tokens]\n",
    "\n",
    "def build_index(docs: list[str], tokenizer) -> tuple[dict, list[int]]:\n",
    "    \"\"\"Build an inverted index from a list of documents.\"\"\"\n",
    "    index = {}\n",
    "    doc_lengths = []\n",
    "    for doc_id, doc in enumerate(docs):\n",
    "        tokens = tokenizer(doc)\n",
    "        doc_lengths.append(len(tokens))\n",
    "        term_counts = Counter(tokens)\n",
    "        for term, count in term_counts.items():\n",
    "            if term not in index:\n",
    "                index[term] = {}\n",
    "            index[term][doc_id] = count\n",
    "    return index, doc_lengths\n",
    "\n",
    "def get_tf(term: str, doc_id: int, index: dict) -> int:\n",
    "    \"\"\"Get term frequency for a term in a document.\"\"\"\n",
    "    if term in index and doc_id in index[term]:\n",
    "        return index[term][doc_id]\n",
    "    return 0\n",
    "\n",
    "def get_df(term: str, index: dict) -> int:\n",
    "    \"\"\"Get document frequency for a term.\"\"\"\n",
    "    if term in index:\n",
    "        return len(index[term])\n",
    "    return 0\n",
    "\n",
    "def bm25_idf(df: int, num_docs: int) -> float:\n",
    "    \"\"\"BM25 IDF formula.\"\"\"\n",
    "    return np.log((num_docs - df + 0.5) / (df + 0.5) + 1)\n",
    "\n",
    "def bm25_tf(tf: int, doc_len: int, avg_doc_len: float, k1: float = 1.2, b: float = 0.75) -> float:\n",
    "    \"\"\"BM25 TF normalization.\"\"\"\n",
    "    return (tf * (k1 + 1)) / (tf + k1 * (1 - b + b * doc_len / avg_doc_len))\n",
    "\n",
    "def score_bm25(query: str, index: dict, num_docs: int, doc_lengths: list[int],\n",
    "               tokenizer, k1: float = 1.2, b: float = 0.75) -> np.ndarray:\n",
    "    \"\"\"Score all documents using BM25.\"\"\"\n",
    "    query_tokens = tokenizer(query)\n",
    "    scores = np.zeros(num_docs)\n",
    "    avg_doc_len = np.mean(doc_lengths) if doc_lengths else 1.0\n",
    "    for token in query_tokens:\n",
    "        df = get_df(token, index)\n",
    "        if df == 0:\n",
    "            continue\n",
    "        idf = bm25_idf(df, num_docs)\n",
    "        if token in index:\n",
    "            for doc_id, tf in index[token].items():\n",
    "                tf_norm = bm25_tf(tf, doc_lengths[doc_id], avg_doc_len, k1, b)\n",
    "                scores[doc_id] += idf * tf_norm\n",
    "    return scores\n",
    "\n",
    "def search_products(query: str, products_df: pd.DataFrame, index: dict,\n",
    "                    doc_lengths: list[int], tokenizer, k: int = 10) -> pd.DataFrame:\n",
    "    \"\"\"Search products and return top-k results.\"\"\"\n",
    "    scores = score_bm25(query, index, len(products_df), doc_lengths, tokenizer)\n",
    "    top_k_idx = np.argsort(-scores)[:k]\n",
    "    results = products_df.iloc[top_k_idx].copy()\n",
    "    results[\"score\"] = scores[top_k_idx]\n",
    "    results[\"rank\"] = range(1, k + 1)\n",
    "    return results\n",
    "\n",
    "print('BM25 functions loaded from HW3!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 2: Understanding Embeddings (15 points)\n",
    "\n",
    "### 2a. Load a local model and generate embeddings (5 pts)\n",
    "\n",
    "Use `sentence-transformers` to load a local embedding model and generate embeddings for a list of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the all-MiniLM-L6-v2 model using SentenceTransformer\n",
    "# Then generate embeddings for each word in the list\n",
    "words = [\"wooden coffee table\", \"oak dining table\", \"red leather sofa\", \"blue area rug\", \"kitchen sink\"]\n",
    "\n",
    "# YOUR CODE HERE\n",
    "model_local = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "embeddings_local = model_local.encode(words)\n",
    "\n",
    "# Print the number of embeddings you generated and the dimension of the embeddings\n",
    "print(f\"Generated {len(embeddings_local)} embeddings\")\n",
    "print(f\"Embedding dimension: {embeddings_local.shape[1]}\")\n",
    "print(f\"First embedding shape: {embeddings_local[0].shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b. Implement cosine similarity and create a similarity matrix (5 pts)\n",
    "\n",
    "Implement cosine similarity from scratch:\n",
    "\n",
    "$$\\text{cosine\\_similarity}(a, b) = \\frac{a \\cdot b}{\\|a\\| \\times \\|b\\|}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement cosine similarity from scratch\n",
    "def cosine_similarity(a, b):\n",
    "    dot_product = np.dot(a, b)\n",
    "    norm_a = np.linalg.norm(a)\n",
    "    norm_b = np.linalg.norm(b)\n",
    "    if norm_a == 0 or norm_b == 0:\n",
    "        return 0.0\n",
    "    return dot_product / (norm_a * norm_b)\n",
    "\n",
    "# Create similarity matrix\n",
    "n = len(words)\n",
    "sim_matrix = np.zeros((n, n))\n",
    "\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        sim_matrix[i, j] = cosine_similarity(embeddings_local[i], embeddings_local[j])\n",
    "\n",
    "# Display as DataFrame\n",
    "pd.DataFrame(sim_matrix, index=words, columns=words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2c. Embed using OpenAI API (5 pts)\n",
    "\n",
    "Use `litellm` to get embeddings from OpenAI's API and compare dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use litellm to get an embedding from OpenAI's text-embedding-3-small model\n",
    "response = litellm.embedding(\n",
    "    model=\"text-embedding-3-small\",\n",
    "    input=[\"wooden coffee table\"]\n",
    ")\n",
    "\n",
    "embedding_openai = response.data[0][\"embedding\"]\n",
    "\n",
    "# Compare the dimension with the local model\n",
    "print(f\"Local model dimension: {embeddings_local.shape[1]}\")\n",
    "print(f\"OpenAI model dimension: {len(embedding_openai)}\")\n",
    "\n",
    "# Calculate ratio\n",
    "ratio = len(embedding_openai) / embeddings_local.shape[1]\n",
    "print(f\"OpenAI embedding is {ratio:.1f}x larger than local embedding\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 3: Batch Embedding Products (20 points)\n",
    "\n",
    "### 3a. Embed a product sample (10 pts)\n",
    "\n",
    "Create a combined text field and embed 5,000 products using the local model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a consistent sample\n",
    "products_sample = products.sample(5000, random_state=42).copy()\n",
    "print(f\"Sample size: {len(products_sample)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a combined text field (product_name + product_class)\n",
    "# Then embed all products using model.encode()\n",
    "\n",
    "# Combine fields - be extremely defensive against NaNs\n",
    "products_sample[\"text\"] = (\n",
    "    products_sample[\"product_name\"].fillna(\"\").astype(str) + \" \" + \n",
    "    products_sample[\"product_class\"].fillna(\"\").astype(str)\n",
    ")\n",
    "\n",
    "# Double check: ensure every single item is a string\n",
    "products_sample[\"text\"] = products_sample[\"text\"].astype(str)\n",
    "\n",
    "# Generate embeddings (this may take a minute)\n",
    "print(\"Generating embeddings... (this may take 1-2 minutes)\")\n",
    "product_embeddings = model_local.encode(products_sample[\"text\"].tolist(), show_progress_bar=True)\n",
    "\n",
    "print(f\"Created embeddings with shape: {product_embeddings.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3b. Save and load embeddings (5 pts)\n",
    "\n",
    "Save embeddings to a `.npy` file so you don't have to recompute them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save embeddings to temp/hw4_embeddings.npy\n",
    "# Save products_sample to temp/hw4_products.csv\n",
    "import os\n",
    "os.makedirs(\"temp\", exist_ok=True)\n",
    "\n",
    "# Save\n",
    "np.save(\"temp/hw4_embeddings.npy\", product_embeddings)\n",
    "products_sample.to_csv(\"temp/hw4_products.csv\", index=False)\n",
    "\n",
    "# Then load them back and verify they match\n",
    "loaded_embeddings = np.load(\"temp/hw4_embeddings.npy\")\n",
    "loaded_products = pd.read_csv(\"temp/hw4_products.csv\")\n",
    "\n",
    "print(f\"Embeddings shape: {loaded_embeddings.shape}\")\n",
    "print(f\"Products shape: {loaded_products.shape}\")\n",
    "\n",
    "if np.allclose(product_embeddings, loaded_embeddings):\n",
    "    print(\"Embeddings match perfectly!\")\n",
    "else:\n",
    "    print(\"Embeddings do NOT match!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3c. Cost estimation (5 pts)\n",
    "\n",
    "Estimate the cost to embed all 43K products using OpenAI's API.\n",
    "\n",
    "**Pricing**: text-embedding-3-small costs ~$0.02 per 1 million tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use tiktoken to count actual tokens in the sample\n",
    "import tiktoken\n",
    "\n",
    "try:\n",
    "    encoding = tiktoken.encoding_for_model(\"text-embedding-3-small\")\n",
    "except:\n",
    "    encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "# Count tokens for all texts in the sample\n",
    "# Calculate tokens per product\n",
    "texts = products_sample[\"text\"].tolist()\n",
    "total_tokens_sample = sum(len(encoding.encode(text)) for text in texts)\n",
    "avg_tokens_per_product = total_tokens_sample / len(products_sample)\n",
    "\n",
    "print(f\"Total tokens in sample (5,000 products): {total_tokens_sample:,}\")\n",
    "print(f\"Average tokens per product: {avg_tokens_per_product:.2f}\")\n",
    "\n",
    "# Then extrapolate to estimate cost for the full dataset (43K products)\n",
    "total_products = len(products)\n",
    "estimated_total_tokens = total_products * avg_tokens_per_product\n",
    "\n",
    "cost_per_1m_tokens = 0.02  # $0.02 per 1M tokens\n",
    "estimated_cost = (estimated_total_tokens / 1_000_000) * cost_per_1m_tokens\n",
    "\n",
    "print(f\"Estimated total tokens for {total_products:,} products: {estimated_total_tokens:,.0f}\")\n",
    "print(f\"Estimated cost to embed full dataset: ${estimated_cost:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 4: Semantic Search (25 points)\n",
    "\n",
    "### 4a. Implement semantic search (15 pts)\n",
    "\n",
    "Implement a semantic search function from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement batch cosine similarity for efficiency\n",
    "def batch_cosine_similarity(query_emb, doc_embs):\n",
    "    \"\"\"\n",
    "    Calculate cosine similarity between a query embedding and many document embeddings.\n",
    "    \n",
    "    Args:\n",
    "        query_emb: 1D array of shape (embedding_dim,)\n",
    "        doc_embs: 2D array of shape (num_docs, embedding_dim)\n",
    "        \n",
    "    Returns:\n",
    "        1D array of similarity scores\n",
    "    \"\"\"\n",
    "    # Normalize query\n",
    "    norm_query = np.linalg.norm(query_emb)\n",
    "    if norm_query == 0:\n",
    "        return np.zeros(doc_embs.shape[0])\n",
    "    query_emb_norm = query_emb / norm_query\n",
    "    \n",
    "    # Normalize docs (can be pre-computed, but we do it here for simplicity)\n",
    "    # Note: For strict cosine similarity, we divide dot product by product of norms\n",
    "    # Equivalent to dot product of normalized vectors\n",
    "    norm_docs = np.linalg.norm(doc_embs, axis=1)\n",
    "    # Avoid division by zero\n",
    "    norm_docs[norm_docs == 0] = 1e-10\n",
    "    \n",
    "    # Calculate dot product\n",
    "    dot_products = np.dot(doc_embs, query_emb)\n",
    "    \n",
    "    # Divide by norms\n",
    "    scores = dot_products / (norm_query * norm_docs)\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement semantic search\n",
    "def semantic_search(query, model, doc_embeddings, products_df, k=10):\n",
    "    \"\"\"\n",
    "    Search products using semantic embeddings.\n",
    "    \n",
    "    Args:\n",
    "        query: Search query string\n",
    "        model: SentenceTransformer model\n",
    "        doc_embeddings: Numpy array of product embeddings\n",
    "        products_df: DataFrame of products (must match order of embeddings)\n",
    "        k: Number of results to return\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with top-k products and similarity scores\n",
    "    \"\"\"\n",
    "    # 1. Embed the query\n",
    "    query_emb = model.encode(query)\n",
    "    \n",
    "    # 2. Calculate similarities\n",
    "    scores = batch_cosine_similarity(query_emb, doc_embeddings)\n",
    "    \n",
    "    # 3. Get top-k results\n",
    "    top_k_idx = np.argsort(-scores)[:k]\n",
    "    \n",
    "    # 4. Return results\n",
    "    results = products_df.iloc[top_k_idx].copy()\n",
    "    results[\"score\"] = scores[top_k_idx]\n",
    "    results[\"rank\"] = range(1, k + 1)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test semantic search\n",
    "query = \"modern coffee table\"\n",
    "results = semantic_search(query, model_local, product_embeddings, products_sample)\n",
    "\n",
    "print(f\"Query: {query}\")\n",
    "results[[\"product_name\", \"product_class\", \"score\"]].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4b. Evaluate and compare BM25 vs semantic search (10 pts)\n",
    "\n",
    "Implement Recall@k and compare the two search methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_recall_at_k(relevant_ids, retrieved_ids, k=10):\n",
    "    \"\"\"\n",
    "    Calculate Recall@k.\n",
    "    \n",
    "    Args:\n",
    "        relevant_ids: Set of relevant product IDs (ground truth)\n",
    "        retrieved_ids: List of retrieved product IDs (ordered)\n",
    "        k: Cutoff rank\n",
    "        \n",
    "    Returns:\n",
    "        Recall score (0.0 to 1.0)\n",
    "    \"\"\"\n",
    "    if not relevant_ids:\n",
    "        return 0.0\n",
    "    \n",
    "    # Get top-k retrieved\n",
    "    top_k = set(retrieved_ids[:k])\n",
    "    \n",
    "    # Count intersection\n",
    "    intersection = len(top_k.intersection(relevant_ids))\n",
    "    \n",
    "    # Calculate recall\n",
    "    return intersection / len(relevant_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build BM25 index for the SAMPLE (to be fair)\n",
    "index_sample, doc_lengths_sample = build_index(\n",
    "    products_sample[\"product_name\"].fillna(\"\").tolist(), \n",
    "    snowball_tokenize\n",
    ")\n",
    "\n",
    "# Filter queries to those with relevant products IN OUR SAMPLE\n",
    "# 1. Get all relevant product IDs for each query\n",
    "relevant_map = labels[labels[\"grade\"] > 0].groupby(\"query_id\")[\"product_id\"].apply(set).to_dict()\n",
    "\n",
    "# 2. Identify products in our sample\n",
    "sample_product_ids = set(products_sample[\"product_id\"])\n",
    "\n",
    "# 3. Filter queries: keep only if they have at least one relevant product in the sample\n",
    "eval_queries = []\n",
    "for _, row in queries.iterrows():\n",
    "    qid = row[\"query_id\"]\n",
    "    if qid in relevant_map:\n",
    "        # Intersection of relevant docs and our sample\n",
    "        relevant_in_sample = relevant_map[qid].intersection(sample_product_ids)\n",
    "        if relevant_in_sample:\n",
    "            eval_queries.append({\n",
    "                \"query_id\": qid,\n",
    "                \"query\": row[\"query\"],\n",
    "                \"relevant_ids\": relevant_in_sample  # Only judge against what is possible to retrieve\n",
    "            })\n",
    "\n",
    "print(f\"Evaluable queries (with relevant docs in sample): {len(eval_queries)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate both BM25 and semantic search on all filtered queries\n",
    "bm25_recalls = []\n",
    "semantic_recalls = []\n",
    "\n",
    "print(\"Running evaluation...\")\n",
    "for q_item in eval_queries:\n",
    "    query_text = q_item[\"query\"]\n",
    "    ground_truth = q_item[\"relevant_ids\"]\n",
    "    \n",
    "    # 1. BM25 Search\n",
    "    bm25_res = search_products(\n",
    "        query_text, products_sample, index_sample, doc_lengths_sample, \n",
    "        snowball_tokenize, k=10\n",
    "    )\n",
    "    bm25_recall = calculate_recall_at_k(ground_truth, bm25_res[\"product_id\"].tolist(), k=10)\n",
    "    bm25_recalls.append(bm25_recall)\n",
    "    \n",
    "    # 2. Semantic Search\n",
    "    # (We could check if we already embedded the query? No, just embed it)\n",
    "    sem_res = semantic_search(query_text, model_local, product_embeddings, products_sample, k=10)\n",
    "    sem_recall = calculate_recall_at_k(ground_truth, sem_res[\"product_id\"].tolist(), k=10)\n",
    "    semantic_recalls.append(sem_recall)\n",
    "\n",
    "# Average Recall@10\n",
    "avg_bm25_recall = np.mean(bm25_recalls)\n",
    "avg_sem_recall = np.mean(semantic_recalls)\n",
    "\n",
    "print(f\"BM25 Average Recall@10: {avg_bm25_recall:.4f}\")\n",
    "print(f\"Semantic Search Average Recall@10: {avg_sem_recall:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "methods = [\"BM25\", \"Semantic Search\"]\n",
    "scores = [avg_bm25_recall, avg_sem_recall]\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(methods, scores, color=[\"skyblue\", \"salmon\"])\n",
    "plt.ylabel(\"Recall@10\")\n",
    "plt.title(\"Search Performance Comparison (BM25 vs Semantic)\")\n",
    "plt.ylim(0, max(scores) * 1.2)\n",
    "for i, v in enumerate(scores):\n",
    "    plt.text(i, v + 0.01, f\"{v:.4f}\", ha=\"center\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 5: Compare Embedding Models (20 points)\n",
    "\n",
    "### 5a. Embed products with two different models (10 pts)\n",
    "\n",
    "Compare embeddings from:\n",
    "- `BAAI/bge-base-en-v1.5`\n",
    "- `sentence-transformers/all-mpnet-base-v2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the two embedding models\n",
    "print(\"Loading BGE model...\")\n",
    "model_bge = SentenceTransformer(\"BAAI/bge-base-en-v1.5\")\n",
    "\n",
    "print(\"Loading MPNet model...\")\n",
    "model_mpnet = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\")\n",
    "\n",
    "print(\"Models loaded!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed products with both models\n",
    "print(\"Embedding with BGE... (this may take 2-3 minutes)\")\n",
    "embeddings_bge = model_bge.encode(products_sample[\"text\"].tolist(), show_progress_bar=True)\n",
    "\n",
    "print(\"Embedding with MPNet... (this may take 2-3 minutes)\")\n",
    "embeddings_mpnet = model_mpnet.encode(products_sample[\"text\"].tolist(), show_progress_bar=True)\n",
    "\n",
    "print(f\"BGE shape: {embeddings_bge.shape}\")\n",
    "print(f\"MPNet shape: {embeddings_mpnet.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5b. Compare search results between models (10 pts)\n",
    "\n",
    "Evaluate both models on the same queries and analyze differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate both models on all queries\n",
    "bge_recalls = []\n",
    "mpnet_recalls = []\n",
    "\n",
    "print(\"Evaluating BGE and MPNet...\")\n",
    "for q_item in eval_queries:\n",
    "    query_text = q_item[\"query\"]\n",
    "    ground_truth = q_item[\"relevant_ids\"]\n",
    "    \n",
    "    # BGE Search\n",
    "    # Note: BGE often requires a prompt like \"Represent this sentence for searching relevant passages:\"\n",
    "    # But for simplicity/standard usage in sentence-transformers we often just encode\n",
    "    # The model card says for queries: \"Represent this sentence for searching relevant passages: \"\n",
    "    # Let's add the instruction for BGE queries as recommended\n",
    "    bge_query_emb = model_bge.encode(\"Represent this sentence for searching relevant passages: \" + query_text)\n",
    "    bge_scores = batch_cosine_similarity(bge_query_emb, embeddings_bge)\n",
    "    bge_top_k = np.argsort(-bge_scores)[:10]\n",
    "    bge_recall = calculate_recall_at_k(ground_truth, products_sample.iloc[bge_top_k][\"product_id\"].tolist(), k=10)\n",
    "    bge_recalls.append(bge_recall)\n",
    "    \n",
    "    # MPNet Search\n",
    "    mpnet_query_emb = model_mpnet.encode(query_text)\n",
    "    mpnet_scores = batch_cosine_similarity(mpnet_query_emb, embeddings_mpnet)\n",
    "    mpnet_top_k = np.argsort(-mpnet_scores)[:10]\n",
    "    mpnet_recall = calculate_recall_at_k(ground_truth, products_sample.iloc[mpnet_top_k][\"product_id\"].tolist(), k=10)\n",
    "    mpnet_recalls.append(mpnet_recall)\n",
    "\n",
    "print(f\"BGE Average Recall@10: {np.mean(bge_recalls):.4f}\")\n",
    "print(f\"MPNet Average Recall@10: {np.mean(mpnet_recalls):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare results for specific queries\n",
    "test_queries = [\"comfortable sofa\", \"star wars rug\", \"modern coffee table\"]\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\n--- Query: {query} ---\")\n",
    "    \n",
    "    # BGE\n",
    "    bge_q_emb = model_bge.encode(\"Represent this sentence for searching relevant passages: \" + query)\n",
    "    bge_scores = batch_cosine_similarity(bge_q_emb, embeddings_bge)\n",
    "    bge_top_1 = products_sample.iloc[np.argmax(bge_scores)][\"product_name\"]\n",
    "    \n",
    "    # MPNet\n",
    "    mpnet_q_emb = model_mpnet.encode(query)\n",
    "    mpnet_scores = batch_cosine_similarity(mpnet_q_emb, embeddings_mpnet)\n",
    "    mpnet_top_1 = products_sample.iloc[np.argmax(mpnet_scores)][\"product_name\"]\n",
    "    \n",
    "    print(f\"BGE Top Result:   {bge_top_1}\")\n",
    "    print(f\"MPNet Top Result: {mpnet_top_1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model comparison with a scatter plot\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(bge_recalls, mpnet_recalls, alpha=0.5)\n",
    "plt.xlabel(\"BGE Recall@10\")\n",
    "plt.ylabel(\"MPNet Recall@10\")\n",
    "plt.title(\"Per-Query Recall Comparison\")\n",
    "plt.plot([0, 1], [0, 1], \"r--\")  # Diagonal line\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 6: Git Submission (5 points)\n",
    "\n",
    "Submit your work using the Git workflow:\n",
    "\n",
    "- [ ] Create a new branch called `homework-4`\n",
    "- [ ] Commit your work with a meaningful message\n",
    "- [ ] Push to GitHub\n",
    "- [ ] Create a Pull Request\n",
    "- [ ] Merge the PR to main\n",
    "- [ ] Submit the `.ipynb` file on Blackboard\n",
    "\n",
    "The TA will verify your submission by checking the merged PR on GitHub."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}